{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting list of similar concepts\n",
    "\n",
    "### This notebook is based on 'Similarity metrics but it iterates over multiple input concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Semantic Similarity\n",
    "\n",
    "This program measure the similarity between pairs of words from the McRae's dataset. First it uses the HD Computing approach and then compares it with similarity metrics from the NLTK library.\n",
    "\n",
    "### Importing libraries and HD computing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('wordnet_ic')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "%run HDComputing_basics.ipynb\n",
    "\n",
    "#pathh = '../McRaedataset/'\n",
    "pathh = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TranslateFeats(ListFeat):\n",
    "    \"It receives a list of features such as ['is_blue', 'is_rectangular'] and it returns: [['color','blue'], ['shape','rectangular']\"\n",
    "    # Dataframe for excel document\n",
    "    df = pd.read_excel(pathh + 'FEATS_brm.xlsx') #../McRaedataset/FEATS_brm.xlsx')\n",
    "    ListPairs = []\n",
    "    for feat in ListFeat:\n",
    "        # Row for feature...\n",
    "        row = df.loc[df['Feature'] == feat]       \n",
    "        # Look for values in vec_feat and vec_value\n",
    "        ListPairs.append([str(row['feat_name'].tolist()[0]), str(row['feat_value'].tolist()[0])])       \n",
    "    return ListPairs\n",
    "\n",
    "def ReadDefinitions():\n",
    "    \"Given an xlsx file it returns all the concepts feature values as they appear in the original dataset\"\n",
    "    #Dataframe for excel document\n",
    "    df = pd.read_excel( pathh + 'CONCS_FEATS_concstats_brm.xlsx') #../McRaeDataset/CONCS_FEATS_concstats_brm.xlsx') #MINI_\n",
    "    #Create a list with all concept names\n",
    "    names = set(df['Concept'])\n",
    "    # Extract list of features for each name\n",
    "    Concepts = []\n",
    "    for n in names:\n",
    "        row = df.loc[df['Concept'] == n]\n",
    "        Concepts.append([str(n), map(str,list(row['Feature']))])\n",
    "    return Concepts\n",
    "\n",
    "def ClosestConcepts (concept, nc):\n",
    "    \"Given a concept label this function reads the distance matrix from McRae's and returns the 'nc' closests concepts in a list\"\n",
    "    # Excel document to data frame...\n",
    "    try:\n",
    "        df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','1st_200') #../McRaeDataset/cos_matrix_brm_IFR.xlsx', '1st_200')\n",
    "        ordered = df.sort_values(by=concept, ascending=False)[['CONCEPT', concept]]\n",
    "    except: \n",
    "        try:\n",
    "            df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','2nd_200') # ('../McRaeDataset/cos_matrix_brm_IFR.xlsx', '2nd_200')\n",
    "            ordered = df.sort_values(by=concept, ascending=False)[['CONCEPT', concept]]\n",
    "        except:\n",
    "            df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','last_141') #('../McRaeDataset/cos_matrix_brm_IFR.xlsx', 'last_141')\n",
    "            ordered = df.sort_values(by=concept, ascending=False)[['CONCEPT', concept]]\n",
    "    \n",
    "    L1 = list(ordered['CONCEPT'][0:nc])\n",
    "    L1 = map(str, L1)\n",
    "    L2 = zip(L1,list(ordered[concept][0:nc]))\n",
    "    L2 = map(list, L2)\n",
    "    \n",
    "    return L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating definitions dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDictionary():\n",
    "    global Dict_defs\n",
    "    data = ReadDefinitions()\n",
    "    for concept in data:\n",
    "        Dict_defs[concept[0]] = TranslateFeats(concept[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing ID vectors into memory\n",
    "\n",
    "### Memory functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_list (L):\n",
    "    \"Recursive function that flats a list of lists (at any level)\"\n",
    "    if L == []:\n",
    "        return L\n",
    "    if type(L[0]) is list:\n",
    "        return flat_list(L[0]) + flat_list(L[1:])\n",
    "    return L[:1] + flat_list(L[1:])\n",
    "\n",
    "def SaveConcepts(Dic):\n",
    "    \"\"\"Given a definitions dictionary it stores in memory the entire set of concepts in the dictionary (including feature vectors)\"\"\"\n",
    "    keys = Dic.keys()\n",
    "    vals = Dic.values()\n",
    "    all_concepts = list(set(flat_list(vals) + keys))\n",
    "    # Process for storing list of concepts in memory\n",
    "    for concept in all_concepts:\n",
    "        HDvector(N,concept) #This creates an object and store it in memory\n",
    "        \n",
    "def FeatureVectors(Dic):\n",
    "    \"It extract from the definition dictionary all the feature type vectors ('is','has','color', etc...)\"\n",
    "    global feature_vectors\n",
    "    featt = []\n",
    "    vals = Dic.values()\n",
    "    for l in vals:\n",
    "        for p in l:\n",
    "            featt.append(p[0])\n",
    "    feature_vectors = list(set(featt))\n",
    "    \n",
    "def CreateSemanticPointer (PairList):\n",
    "    \"Turns list as [[feat1,feat_val],[feat2,feat_val],[feat3,feat_val]] into vector feat1*feat_val + feat2*feat_val ...\"\n",
    "    vecs = []\n",
    "    for pair in PairList:\n",
    "        vecs.append(Dict[pair[0]] * Dict[pair[1]])\n",
    "    return ADD(vecs)\n",
    "\n",
    "def SaveDefinitions(Dic):\n",
    "    \"\"\"Given the definitions dictionary, and having all its concepts previously stored in memory, this functions\n",
    "       creates a definition vector (semantic pointer) using HD operations and assign it as a pointer to an \n",
    "       object vector (ID vector).\"\"\"\n",
    "    global feature_vectors\n",
    "    # Going through all elements in dictionary\n",
    "    for key, value in Dic.iteritems():\n",
    "        Dict[key].setPointer(CreateSemanticPointer(value))\n",
    "        \n",
    "def NormalizeHammDist (Dist_list):\n",
    "    \"Given a distance list of the form [['name', dist], ['name', dist], ... ], it normalize each distance and return a list with the same form\"\n",
    "    for i in range(len(Dist_list)):\n",
    "        Dist_list[i][1] = round( 1. - Dist_list[i][1] / float(N * 0.5), 3 ) #aqui meterle thr... 0.45...0.6??\n",
    "    return Dist_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of initialization\n",
      "End of encoding\n"
     ]
    }
   ],
   "source": [
    "def Init_mem():\n",
    "    init()\n",
    "    thr = 0.4 * N\n",
    "    # Read dataset and create definition dictionary\n",
    "    CreateDictionary()\n",
    "    # Feature vectors\n",
    "    FeatureVectors(Dict_defs)\n",
    "    # Save concepts into memory (ID vectors)\n",
    "    SaveConcepts(Dict_defs)\n",
    "    # Associate definitions to concepts into memory (SP vectors)\n",
    "    SaveDefinitions(Dict_defs)\n",
    "    print \"End of encoding\"\n",
    "\n",
    "Init_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity using NLTK library\n",
    "\n",
    "### Auxiliar functions for similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "def get_concepts_list ():\n",
    "    \"Returns a list of strings: the names of the concepts\"\n",
    "    df = pd.read_excel(pathh + 'CONCS_Synset_brm.xlsx') #../McRaedataset/CONCS_Synset_brm.xlsx')\n",
    "    return map(str, list(df['Concept']))\n",
    "    \n",
    "def get_synset (concept):\n",
    "    \"Given a concept name (string) it returns its synset (string)\"\n",
    "    # Dataframe for excel document\n",
    "    df = pd.read_excel(pathh + 'CONCS_Synset_brm.xlsx') #../McRaedataset/CONCS_Synset_brm.xlsx')\n",
    "    row = df.loc[df['Concept'] == concept]\n",
    "    return str(list(row['Synset'] )[0])\n",
    "\n",
    "def Apply_sim_metric ( similarity_metric, num, in_concept, corpus = None):\n",
    "    \"Given a similarity_metric function it returns a list of the num closest concepts to 'concept'\"\n",
    "    dist_list = []\n",
    "    for c in Concepts:\n",
    "        c_synset = wn.synset( get_synset(c) )\n",
    "        if corpus:\n",
    "            dist_list.append([c, round(similarity_metric(in_concept, c_synset, corpus), 3) ])\n",
    "        else:\n",
    "            dist_list.append([c, round(similarity_metric(in_concept, c_synset), 3) ])\n",
    "    return sorted(dist_list, key = lambda r : r[1], reverse = True ) [:num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting list of closest vectors (HDcomputing and from Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting with... table\n",
      "HDC_sim:  [['table', 0], ['stool_(furniture)', 3925], ['desk', 4080], ['cabinet', 4097], ['chair', 4107], ['peg', 4168], ['spatula', 4175], ['tray', 4244], ['sofa', 4301], ['pot', 4338]]\n",
      "DatSet_sim:  [['table', 1.0], ['barrel', 0.514], ['stool_(furniture)', 0.492], ['cabinet', 0.443], ['chair', 0.425], ['peg', 0.424], ['desk', 0.405], ['board_(wood)', 0.379], ['stick', 0.348], ['raft', 0.336]]\n",
      "\n",
      "\n",
      "Starting with... shoes\n",
      "HDC_sim:  [['shoes', 0], ['boots', 3211], ['belt', 4117], ['slippers', 4163], ['coat', 4240], ['sandals', 4240], ['socks', 4241], ['gloves', 4392], ['trousers', 4493]]\n",
      "DatSet_sim:  [['shoes', 1.0], ['boots', 0.62], ['sandals', 0.382], ['belt', 0.377], ['saddle', 0.315], ['whip', 0.306], ['football', 0.295], ['gloves', 0.269], ['socks', 0.227], ['slippers', 0.204]]\n",
      "\n",
      "\n",
      "Starting with... chair\n",
      "HDC_sim:  [['chair', 0], ['sofa', 3456], ['stool_(furniture)', 3624], ['couch', 3760], ['bench', 4002], ['rocker', 4025], ['table', 4107], ['desk', 4163], ['gopher', 4439], ['cabinet', 4466]]\n",
      "DatSet_sim:  [['chair', 1.0], ['stool_(furniture)', 0.611], ['couch', 0.568], ['sofa', 0.546], ['bench', 0.511], ['table', 0.425], ['desk', 0.407], ['donkey', 0.321], ['rocker', 0.319], ['dog', 0.301]]\n",
      "\n",
      "\n",
      "Starting with... spoon\n",
      "HDC_sim:  [['spoon', 0], ['ladle', 3294], ['spatula', 3738], ['fork', 3766], ['colander', 3841], ['tongs', 3984], ['peg', 4077], ['pot', 4141], ['grater', 4183], ['knife', 4200]]\n",
      "DatSet_sim:  [['spoon', 1.0], ['fork', 0.546], ['ladle', 0.411], ['spatula', 0.395], ['tongs', 0.35], ['colander', 0.336], ['bowl', 0.292], ['strainer', 0.282], ['dish', 0.282], ['peg', 0.28]]\n",
      "\n",
      "\n",
      "Starting with... bed\n",
      "HDC_sim:  [['bed', 0], ['sofa', 4070], ['couch', 4102], ['pajamas', 4227], ['nightgown', 4233], ['pillow', 4351], ['robe', 4378], ['cushion', 4396], ['mink_(coat)', 4473]]\n",
      "DatSet_sim:  [['bed', 1.0], ['pillow', 0.374], ['sofa', 0.323], ['couch', 0.322], ['cushion', 0.284], ['mink_(coat)', 0.158], ['nightgown', 0.152], ['pajamas', 0.149], ['robe', 0.146], ['slippers', 0.145]]\n",
      "\n",
      "\n",
      "Starting with... airplane\n",
      "HDC_sim:  [['airplane', 0], ['jet', 2425], ['train', 3823], ['helicopter', 3870], ['rocket', 3970], ['trolley', 4141], ['hawk', 4181], ['raven', 4185], ['beetle', 4190], ['taxi', 4205]]\n",
      "DatSet_sim:  [['airplane', 1.0], ['jet', 0.775], ['housefly', 0.476], ['falcon', 0.471], ['moth', 0.469], ['hornet', 0.465], ['butterfly', 0.452], ['pigeon', 0.446], ['bat_(animal)', 0.443], ['hawk', 0.438]]\n",
      "\n",
      "\n",
      "Starting with... hose\n",
      "HDC_sim:  [['hose', 0], ['pen', 4261], ['spatula', 4318], ['pipe_(plumbing)', 4326], ['peg', 4369], ['bucket', 4371], ['dish', 4403], ['banana', 4405], ['bin_(waste)', 4409], ['wheel', 4428]]\n",
      "DatSet_sim:  [['hose', 1.0], ['asparagus', 0.388], ['vine', 0.37], ['zucchini', 0.344], ['wand', 0.341], ['baton', 0.338], ['alligator', 0.312], ['pipe_(plumbing)', 0.31], ['pen', 0.31], ['cucumber', 0.295]]\n",
      "\n",
      "\n",
      "Starting with... train\n",
      "HDC_sim:  [['train', 0], ['jet', 3150], ['scooter', 3817], ['airplane', 3823], ['bus', 3903], ['car', 3915], ['dunebuggy', 3957], ['motorcycle', 3957], ['van', 4125], ['taxi', 4166]]\n",
      "DatSet_sim:  [['train', 1.0], ['jet', 0.446], ['scooter', 0.349], ['bus', 0.348], ['subway', 0.341], ['car', 0.337], ['airplane', 0.291], ['motorcycle', 0.272], ['dunebuggy', 0.25], ['skateboard', 0.205]]\n"
     ]
    }
   ],
   "source": [
    "#Test_Concepts = ['hose','piano','rope', 'sword', 'train', 'toilet'] #['airplane', 'apple', 'bed', 'coin']\"\n",
    "# Por familiaridad... \n",
    "Test_Concepts =  ['table', 'shoes', 'chair','spoon', 'bed','airplane','hose','train']\n",
    "\n",
    "num_concepts_1 = 10 # 6 #20\n",
    "num_concepts_2 = 10 #20\n",
    "\n",
    "# List of concepts for NLTK similarity metrics\n",
    "Concepts = get_concepts_list() \n",
    "\n",
    "for test_concept in Test_Concepts:\n",
    "    print \"\\n\\nStarting with...\", test_concept\n",
    "    # Asking closest concept of another concept's definition...\n",
    "    HDC_sim = HDvector.getLabelSP(Dict[test_concept].getPointer())[:num_concepts_1]\n",
    "    #HDC_sim = NormalizeHammDist(HDC_sim)\n",
    "    DatSet_sim = ClosestConcepts(test_concept, num_concepts_1)\n",
    "    print \"HDC_sim: \", HDC_sim\n",
    "    print \"DatSet_sim: \", DatSet_sim\n",
    "\n",
    "    concept = wn.synset( get_synset(test_concept) )\n",
    "    # Path similarity\n",
    "    #Path_sim = Apply_sim_metric(wn.path_similarity, num_concepts_2, concept)\n",
    "    #LC_sim = Apply_sim_metric(wn.lch_similarity, num_concepts_2, concept ) \n",
    "    WUP_sim = Apply_sim_metric(wn.wup_similarity, num_concepts_2, concept )\n",
    "    print \"WUP_sim: \", WUP_sim\n",
    "    # Information Content\n",
    "    #Res_sim = Apply_sim_metric(wn.res_similarity, num_concepts_2, concept, brown_ic)\n",
    "    JC_sim = Apply_sim_metric(wn.jcn_similarity, num_concepts_2, concept, brown_ic)\n",
    "    #Lin_sim = Apply_sim_metric(wn.lin_similarity, num_concepts_2, concept, brown_ic)\n",
    "    print \"JC_sim: \", JC_sim\n",
    "    \n",
    "    # SETS... Performing Unions and Intersections\n",
    "    # Creating name sets\n",
    "    HDC_names = set([x[0] for x in HDC_sim])\n",
    "    DatSet_names = set([x[0] for x in DatSet_sim])\n",
    "    #Path_names = set([x[0] for x in Path_sim])\n",
    "    #LC_names = set([x[0] for x in LC_sim])\n",
    "    WUP_names = set([x[0] for x in WUP_sim])\n",
    "    #Res_names = set([x[0] for x in Res_sim])\n",
    "    JC_names = set([x[0] for x in JC_sim])\n",
    "    #Lin_names = set([x[0] for x in Lin_sim])\n",
    "    \n",
    "    # Intersection between HDC and Dataset\n",
    "    union_1 = HDC_names.union(DatSet_names)  #Era intersección... pero así esta mejor...\n",
    "\n",
    "    # Union of intersecion Path and intersection IC...\n",
    "    #PathInt = Path_names.intersection(LC_names, WUP_names)\n",
    "    #ICInt = Res_names.intersection(JC_names, Lin_names)\n",
    "    union_2 = WUP_names.union(JC_names)\n",
    "\n",
    "    # Intersection between all NLTK metrics\n",
    "    #union_3 = PathInt.intersection(ICInt)\n",
    "\n",
    "    # Intersection all NLTLK metrics and HDC and Dataset\n",
    "    #union_4 = HDC_names.intersection(PathInt, ICInt)\n",
    "    #union_5 = DatSet_names.intersection(PathInt, ICInt)\n",
    "    #union_6 = HDC_names.intersection(DatSet_names, PathInt, ICInt)\n",
    "\n",
    "    # MENOS CONCEPTOS EN LA UNION FINAL ES MEJOR... TAL VEZ PUEDA APLICAR ESTE PROGRAMA A TODOS LOS CONCEPTOS Y \n",
    "    # SELECCIONAR LOS QUE TIENEN LA LONGITUD MENOR... \n",
    "    # Ultimate union...\n",
    "    ult_union = set.union( union_1, union_2 )\n",
    "    print \"\\nUltimate Union for\", test_concept,\": \\n\", ult_union, \"\\nLenght: \", len(ult_union)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
