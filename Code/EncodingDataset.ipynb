{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the knowledge base into Hyperdimensional Vectors\n",
    "\n",
    "In this notebook the functions from the 'HDComputing' notebook are used to encode the McRae dataset. The following functions create an heteroassociative memory in which a knowledge base of Semantic Features representation of concepts is stored.\n",
    "\n",
    "### Importing libraries and HD computing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jobqu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\jobqu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet_ic.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "#Only done once... \n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('wordnet_ic')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "# Declare functions and class...\n",
    "%run HDComputing_basics.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TranslateFeats(ListFeat):\n",
    "    \"It receives a list of features such as ['is_blue', 'is_rectangular'] and it returns: [['color','blue'], ['shape','rectangular']\"\n",
    "    # Dataframe for excel document\n",
    "    df = pd.read_excel(pathh + 'FEATS_brm.xlsx') #../McRaedataset/FEATS_brm.xlsx')\n",
    "    ListPairs = []\n",
    "    for feat in ListFeat:\n",
    "        # Row for feature...\n",
    "        row = df.loc[df['Feature'] == feat]       \n",
    "        # Look for values in vec_feat and vec_value\n",
    "        ListPairs.append([str(row['feat_name'].tolist()[0]), str(row['feat_value'].tolist()[0])])       \n",
    "    return ListPairs\n",
    "\n",
    "def ReadDefinitions():\n",
    "    \"Given an xlsx file it returns all the concepts feature values as they appear in the original dataset\"\n",
    "    #Dataframe for excel document\n",
    "    df = pd.read_excel( pathh + 'CONCS_FEATS_concstats_brm.xlsx') #../McRaeDataset/CONCS_FEATS_concstats_brm.xlsx') #MINI_\n",
    "    #Create a list with all concept names\n",
    "    names = set(df['Concept'])\n",
    "    # Extract list of features for each name\n",
    "    Concepts = []\n",
    "    for n in names:\n",
    "        row = df.loc[df['Concept'] == n]\n",
    "        Concepts.append([str(n), map(str,list(row['Feature']))])\n",
    "    return Concepts\n",
    "\n",
    "def ClosestConcepts (concept, nc):\n",
    "    \"Given a concept label this function reads the distance matrix from McRae's and returns the 'nc' closests concepts in a list\"\n",
    "    # Excel document to data frame...\n",
    "    try:\n",
    "        df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','1st_200') #../McRaeDataset/cos_matrix_brm_IFR.xlsx', '1st_200')\n",
    "        ordered = df.sort_values(by=concept, ascending=False)[['CONCEPT', concept]]\n",
    "    except: \n",
    "        try:\n",
    "            df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','2nd_200') # ('../McRaeDataset/cos_matrix_brm_IFR.xlsx', '2nd_200')\n",
    "            ordered = df.sort_values(by=concept, ascending=False)[['CONCEPT', concept]]\n",
    "        except:\n",
    "            df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','last_141') #('../McRaeDataset/cos_matrix_brm_IFR.xlsx', 'last_141')\n",
    "            ordered = df.sort_values(by=concept, ascending=False)[['CONCEPT', concept]]\n",
    "    \n",
    "    L1 = list(ordered['CONCEPT'][0:nc])\n",
    "    L1 = map(str, L1)\n",
    "    L2 = zip(L1,list(ordered[concept][0:nc]))\n",
    "    L2 = map(list, L2)\n",
    "    \n",
    "    return L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing ID vectors into memory\n",
    "\n",
    "### Creating definitions dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDictionary():\n",
    "    global Dict_defs\n",
    "    data = ReadDefinitions()\n",
    "    for concept in data:\n",
    "        Dict_defs[concept[0]] = TranslateFeats(concept[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_list (L):\n",
    "    \"Recursive function that flats a list of lists (at any level)\"\n",
    "    if L == []:\n",
    "        return L\n",
    "    if type(L[0]) is list:\n",
    "        return flat_list(L[0]) + flat_list(L[1:])\n",
    "    return L[:1] + flat_list(L[1:])\n",
    "\n",
    "def FeatureVectors(Dic):\n",
    "    \"It extract from the definition dictionary all the feature type vectors ('is','has','color', etc...)\"\n",
    "    global feature_vectors\n",
    "    featt = []\n",
    "    vals = Dic.values()\n",
    "    for l in vals:\n",
    "        for p in l:\n",
    "            featt.append(p[0])\n",
    "    feature_vectors = list(set(featt))\n",
    "    \n",
    "    \n",
    "def SaveConcepts(Dic):\n",
    "    \"\"\"Given a definitions dictionary it stores in memory the entire set of concepts in the dictionary (including feature vectors)\"\"\"\n",
    "    keys = Dic.keys()\n",
    "    vals = list( Dic.values() )\n",
    "    all_concepts = list(set(flat_list(vals) + keys))\n",
    "    # Process for storing list of concepts in memory\n",
    "    for concept in all_concepts:\n",
    "        HDvector(N,concept) #This creates an object and store it in memory    \n",
    "    \n",
    "def CreateSemanticPointer (PairList):\n",
    "    \"Turns list as [[feat1,feat_val],[feat2,feat_val],[feat3,feat_val]] into vector feat1*feat_val + feat2*feat_val ...\"\n",
    "    vecs = []\n",
    "    for pair in PairList:\n",
    "        vecs.append(Dict[pair[0]] * Dict[pair[1]])\n",
    "    return ADD(vecs)\n",
    "\n",
    "def SaveDefinitions(Dic):\n",
    "    \"\"\"Given the definitions dictionary, and having all its concepts previously stored in memory, this functions\n",
    "       creates a definition vector (semantic pointer) using HD operations and assign it as a pointer to an \n",
    "       object vector (ID vector).\"\"\"\n",
    "    global feature_vectors\n",
    "    # Going through all elements in dictionary\n",
    "    for key, value in Dic.iteritems():\n",
    "        Dict[key].setPointer(CreateSemanticPointer(value))\n",
    "        \n",
    "def NormalizeHammDist (Dist_list):\n",
    "    \"Given a distance list of the form [['name', dist], ['name', dist], ... ], it normalize each distance and return a list with the same form\"\n",
    "    for i in range(len(Dist_list)):\n",
    "        Dist_list[i][1] = round( 1. - Dist_list[i][1] / float(N), 3 ) \n",
    "    return Dist_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Init_mem():\n",
    "    init()\n",
    "    print(\"Begining to encode dataset...\")\n",
    "    thr = 0.45 * N\n",
    "    # Read dataset and create definition dictionary\n",
    "    CreateDictionary()\n",
    "    # Feature vectors\n",
    "    FeatureVectors(Dict_defs)\n",
    "    # Save concepts into memory (ID vectors)\n",
    "    SaveConcepts(Dict_defs)\n",
    "    # Associate definitions to concepts into memory (SP vectors)\n",
    "    SaveDefinitions(Dict_defs)\n",
    "    print(\"End of encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading similarity from distance matrix (McRae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def McRae_simi (pair_concepts):\n",
    "    \"Given a pair of concepts (in a list) it consults the similarity from the cos_matrix... file\"\n",
    "    try: \n",
    "        df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','1st_200')\n",
    "        return list(df.loc[df['CONCEPT'] == pair_concepts[0]][pair_concepts[1]])[0]\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','2nd_200')\n",
    "            return list(df.loc[df['CONCEPT'] == pair_concepts[0]][pair_concepts[1]])[0]\n",
    "        except:\n",
    "            df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','last_141')\n",
    "            return list(df.loc[df['CONCEPT'] == pair_concepts[0]][pair_concepts[1]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic similarity using NLTK library functions\n",
    "### Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "def get_concepts_list ():\n",
    "    \"Returns a list of strings: the names of the concepts\"\n",
    "    df = pd.read_excel(pathh + 'CONCS_Synset_brm.xlsx') #../McRaedataset/CONCS_Synset_brm.xlsx')\n",
    "    return map(str, list(df['Concept']))\n",
    "    \n",
    "def get_synset (concept):\n",
    "    \"Given a concept name (string) it returns its synset (string)\"\n",
    "    # Dataframe for excel document\n",
    "    df = pd.read_excel(pathh + 'CONCS_Synset_brm.xlsx') #../McRaedataset/CONCS_Synset_brm.xlsx')\n",
    "    row = df.loc[df['Concept'] == concept]\n",
    "    return str(list(row['Synset'] )[0])\n",
    "\n",
    "def apply_sim_metric ( similarity_metric, num, in_concept, corpus = None):\n",
    "    \"Given a similarity_metric function it returns a list of the num closest concepts to 'concept'\"\n",
    "    dist_list = []\n",
    "    for c in Concepts:\n",
    "        c_synset = wn.synset( get_synset(c) )\n",
    "        if corpus:\n",
    "            dist_list.append([c, round(similarity_metric(in_concept, c_synset, corpus), 3) ])\n",
    "        else:\n",
    "            dist_list.append([c, round(similarity_metric(in_concept, c_synset), 3) ])\n",
    "    return sorted(dist_list, key = lambda r : r[1], reverse = True ) [:num]\n",
    "\n",
    "def similarity_fun ( similarity_metric, pair, corpus = None):\n",
    "    \"Given a similarity_metric function it returns a list of the num closest concepts to 'concept'\"\n",
    "    c_synset_1 = wn.synset( get_synset(pair[0]))\n",
    "    c_synset_2 = wn.synset( get_synset(pair[1]))\n",
    "    if corpus:\n",
    "        return round(similarity_metric(c_synset_1, c_synset_2, corpus), 3)\n",
    "    else:\n",
    "        return round(similarity_metric(c_synset_1, c_synset_2), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New similarity metric: Extended Gloss Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "HYPE = 'hypernymy'\n",
    "HYPO = 'hyponymy'\n",
    "GLOSS = 'gloss'\n",
    "MERONYMY = 'meronymy'\n",
    "\n",
    "def glossOverlap(gloss1, gloss2):\n",
    "    # stopws = stopwords.words('english')\n",
    "    stopws = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
    "    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "    'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "    'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', \n",
    "    'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', \n",
    "    'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'then',  \n",
    "    'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each',\n",
    "    'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
    "    'than', 's', 't', 'just', 'don']\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # print \"-\"*50\n",
    "    # for x in stopws:\n",
    "    #  if stemmer.stem_word(x) != x:\n",
    "    #     print x, stemmer.stem_word(x)\n",
    "\n",
    "    def longestOverlap(a, b):\n",
    "        now = [0]*len(b)\n",
    "        bestOverlap = 0\n",
    "        aStart = 0\n",
    "        bStart = 0\n",
    "\n",
    "        nextNonStopWord = [-1]*(len(a)+1)\n",
    "        for i in range(len(a)-1, 0, -1):\n",
    "            if a[i] not in stopws:\n",
    "                nextNonStopWord[i] = i\n",
    "            else:\n",
    "                nextNonStopWord[i] = nextNonStopWord[i+1]\n",
    "\n",
    "        for i in range(1, len(a)):\n",
    "            prev = now\n",
    "            now = [0]*len(b)\n",
    "            if a[i] == '#':\n",
    "                continue\n",
    "            for j in range(1, len(b)):\n",
    "                if b[j] == '#':\n",
    "                    continue\n",
    "                if a[i] == b[j]:\n",
    "                    now[j] = max(now[j], prev[j-1] + 1)\n",
    "                    if a[i] in stopws:\n",
    "                        continue\n",
    "\n",
    "                    overlap = now[j]\n",
    "                    start = i - overlap + 1\n",
    "                    start = nextNonStopWord[start]\n",
    "                    overlap = i - start + 1\n",
    "                    if bestOverlap < overlap:\n",
    "                        bestOverlap = overlap\n",
    "                        aStart = i - overlap + 1\n",
    "                        bStart = j - overlap + 1\n",
    "\n",
    "        return (bestOverlap, aStart, bStart)\n",
    "\n",
    "\n",
    "    regex = ',|\\.|\\s|\\?|\\'|\\\"|!|;|-'\n",
    "    #maybe check what happens if we don't stem the glosses\n",
    "    a1 = ['#'] + [stemmer.stem(x.lower()) for x in re.split(regex, gloss1) if x]\n",
    "    a2 = ['#'] + [stemmer.stem(x.lower()) for x in re.split(regex, gloss2) if x]\n",
    "\n",
    "    score = 0\n",
    "    (overlap, start1, start2) = longestOverlap(a1, a2)\n",
    "    while overlap > 0:\n",
    "        # print overlap\n",
    "        # print a1[start1:start1+overlap]\n",
    "        # print a2[start2:start2+overlap]\n",
    "        a1[start1:start1+overlap] = ['#']\n",
    "        a2[start2:start2+overlap] = ['#']\n",
    "        score += overlap**2\n",
    "        (overlap, start1, start2) = longestOverlap(a1, a2)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def getRelationGloss(synset, r):\n",
    "    synsets = []\n",
    "    if r == HYPE:\n",
    "        synsets = synset.hypernyms()\n",
    "    elif r == HYPO:\n",
    "        synsets = synset.hyponyms()\n",
    "    elif r == GLOSS:\n",
    "        synsets = [synset]\n",
    "    elif r == MERONYMY:\n",
    "        synsets = synset.member_meronyms() + synset.part_meronyms() + synset.substance_meronyms()\n",
    "    else:\n",
    "        #to do: add other relations if necessary\n",
    "        raise Exception(\"Unknown relation %s\" % r)\n",
    "\n",
    "    glosses = [x.definition() for x in synsets]\n",
    "    strr = \" \".join(glosses)\n",
    "    return strr\n",
    "\n",
    "def GlossRelatedness(synset1, synset2):\n",
    "    relpairs = [(GLOSS, GLOSS)] #, (HYPE, HYPE), (HYPO, HYPO), (HYPE, GLOSS), (GLOSS, HYPE)]\n",
    "    ret = 0\n",
    "    for (r1, r2) in relpairs:\n",
    "        gloss1 = getRelationGloss(synset1, r1)\n",
    "        gloss2 = getRelationGloss(synset2, r2)\n",
    "\n",
    "        ret += glossOverlap(gloss1, gloss2)\n",
    "    return ret\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
